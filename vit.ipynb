{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # <- база для нейронок\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm  # <- визуализируем полоску с процессом обучения (прогресс бар + факт: с такадум с арабского это прогресс :) ) \n",
    "from sklearn.model_selection import train_test_split # <- делит данные для тренировки и теста (в нашем случае валидации)\n",
    "from torch.utils.data import Dataset, DataLoader # <- создает датасетики\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image # <- для работы с фото\n",
    "import pandas as pd # <- пандас, база. Работаем с датасетом\n",
    "import wandb\n",
    "import os\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification # <- модель\n",
    "from torcheval.metrics.functional import multiclass_f1_score # <- посчитать метрику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Все обучение модели проводилось на Kaggle, на нем очень удобно работать с GPU (своего не имеем)\n",
    "\n",
    "Но память на кагле не бесконечная, поэтому ее надо иногда очищать, освобожая место от ненужного\n",
    "\n",
    "А без GPU нельзя?\n",
    "\n",
    "- конечно можно, только вот VIT достаточно тяжелая модель и обучаться это все будет без GPU оооочень долго. Но зато какой результат дает VIT! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # <- тут очищаем\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # <- а это надо для дальнейшего использования гпу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VITdataset(Dataset):\n",
    "    def __init__(self, images_paths, images_names, images_indxes, trainable): \n",
    "        self.images_paths = images_paths # <- путь до папки с картинками (у каждого он свой)\n",
    "        self.images_names = images_names # <- название картинок в папке\n",
    "        self.images_indxes = images_indxes # <- метки классов, по задаче их 10\n",
    "        self.processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224') # загружаем предобученную модель\n",
    "        image_mean, image_std = self.processor.image_mean, self.processor.image_std\n",
    "        normalize = v2.Normalize(mean=image_mean, std=image_std)\n",
    "\n",
    "        # trainable - указывает тренировка или валидация\n",
    "        if trainable == True:\n",
    "            # Для обучения делаем аугментацию\n",
    "            self.transform = v2.Compose([\n",
    "                v2.Resize((self.processor.size[\"height\"], self.processor.size[\"width\"])),\n",
    "                v2.RandomHorizontalFlip(0.4),\n",
    "                v2.RandomVerticalFlip(0.1),\n",
    "                v2.RandomApply(transforms=[v2.RandomRotation(degrees=(0, 90))], p=0.5),\n",
    "                v2.RandomApply(transforms=[v2.ColorJitter(brightness=.3, hue=.1)], p=0.3),\n",
    "                v2.RandomApply(transforms=[v2.GaussianBlur(kernel_size=(5, 9))], p=0.3),\n",
    "                v2.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "        elif trainable == False:\n",
    "            # Здесь подгоняем картинку под требования модели (размер картинки и остальное)\n",
    "            self.transform = v2.Compose([\n",
    "                v2.Resize((self.processor.size[\"height\"], self.processor.size[\"width\"])),\n",
    "                v2.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "\n",
    "    # Чтобы понимать сколько элементов в эпохе, чтобы понять когда даталодеру закончить итерироваться\n",
    "    def __len__(self):\n",
    "        return len(self.images_indxes)\n",
    "\n",
    "    # Тут возвращаем картинку и метку (класс), а даталодер сформирует батчи, в которых первый элемент - картинка, второй - массив из меток \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.images_paths, self.images_names[idx][0]))\n",
    "        image = self.transform(image)\n",
    "        return image, self.images_indxes[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные моменты для кода выше:\n",
    "\n",
    "Почему VIT?\n",
    "\n",
    "- предобучен на разных ImageNet-21k (в сумме около 15млн картинок!!)\n",
    "\n",
    "Почему не с нуля, а предобученную? \n",
    "\n",
    "- сделает качество нашей будущей модели выше, за счет огромного количества данных, на которых была обучена модель\n",
    "\n",
    "\n",
    "Зачем используем аугментацию?\n",
    "\n",
    "- увеличим количество картинок для тренировки\n",
    "\n",
    "- а еще! Аугментация сделает нашу модель более устойчивой к шумам и небольшим изменениям данных, это еще предотвратит переобучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Меняем голову"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А зачем?\n",
    "\n",
    "Во-первых, наша задача отличается от той, которую решала VIT, как минимум количеством классов, значит, надо что-то менять\n",
    "\n",
    "Во-вторых, дообучим модель на наших данных (конкретно fine-tuning (существуют еще transfer learning и linear probing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, input_dim) # <- линейный слой раз\n",
    "        self.gelu1 = nn.GELU() # <- функция активации\n",
    "        self.linear2 = nn.Linear(input_dim, output_dim) # <- линейный слой двас\n",
    "\n",
    "    # Проход вперед\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights & Biases или же WandB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшего беспроблемного запуска кода нужно зарегистрироваться на платформе <Weights & Biases>, там можно смотреть на то, как падает лосс при увеличении эпох, и посмотреть на f1-метрику. Зарегистрировать нужно, чтобы получить API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прикладываю визуализацию последней версии модели (если смотреть через GitHub - фото будет недоступно, оно находится в <последняя модель.png>) \n",
    "\n",
    "<image src=\"последняя модель.png\" alt=\"Текст с описанием картинки\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(project='vit-image-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT():\n",
    "    def __init__(self,):\n",
    "        self.model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.model.classifier = Head(self.model.classifier.in_features, 10) # <- меняем голову на нашу\n",
    "        self.model = self.model.to(device) # <- перекладываем на GPUшку\n",
    "\n",
    "    # Для обучения\n",
    "    def train(self, dataset, validation, epochs):\n",
    "        # Adam - метод оптимизации нейронки (для RNN и Transformers подходит), model.parameters() - передаем параметры, lr - длина шага град спуска, \n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-7)\n",
    "        criterion = nn.CrossEntropyLoss() # <- функция потерь\n",
    "        lambda_lr = lambda epoch: 0.4 ** epoch\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_lr)\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train() # сейчас будет тренировать модель\n",
    "            # передаем тензора на куду\n",
    "            for images, targets in tqdm(dataset, desc='Training', colour=\"cyan\"):\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad() # зануляем старые градиенты, чтобы новые не суммировать со старыми\n",
    "                model_output = self.model(images).logits\n",
    "                loss = criterion(model_output, targets)\n",
    "                loss.backward() # мы уже на проходе назад (а когда-то выше был проход вперед)\n",
    "                optimizer.step() # делаем шаг градиентного спуска, то есть обновляем веса\n",
    "\n",
    "                wandb.log({\"loss\": loss})\n",
    "            scheduler.step()\n",
    "\n",
    "            # Теперь оформим валидацию\n",
    "            self.model.eval()\n",
    "            F1_sum = []\n",
    "            # на тестовой выборке нам не нужно градиенты, поэтому сделаем так, чтобы быстрее считалось\n",
    "            with torch.no_grad():\n",
    "                for images, targets in tqdm(validation, desc='Validation', colour=\"green\"):\n",
    "                    images = images.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    model_output = self.model(images).logits\n",
    "                    pred_class = torch.argmax(model_output, dim=1)\n",
    "\n",
    "                    F1_metric = multiclass_f1_score(pred_class, targets, num_classes=10)\n",
    "                    F1_sum.append(F1_metric.item())\n",
    "\n",
    "            F1_sum = sum(F1_sum)/len(F1_sum)\n",
    "            wandb.log({\"F1_metric\": F1_sum})\n",
    "\n",
    "            wandb.log({\"epoch\": epoch + 1})\n",
    "\n",
    "        PATH = '/kaggle/working/Vit.pt'\n",
    "        torch.save(self.model.state_dict(), PATH)\n",
    "\n",
    "    # И вот мы уже на методе предсказания классов для теста\n",
    "    def predict(self, path_to_images):\n",
    "        names = os.listdir(path_to_images)\n",
    "        processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        image_mean, image_std = processor.image_mean, processor.image_std\n",
    "\n",
    "        normalize = v2.Normalize(mean=image_mean, std=image_std)\n",
    "        transform = v2.Compose([\n",
    "            v2.Resize((processor.size[\"height\"], processor.size[\"width\"])),\n",
    "            v2.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "        model_result = []\n",
    "        for name in tqdm(names, desc='Prediction', colour=\"red\"):\n",
    "            image = Image.open(os.path.join(path_to_images, name))\n",
    "            image = transform(image)\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            image = image.to(device)\n",
    "            pred_class = torch.argmax(self.model(image).logits).item()\n",
    "            model_result.append(pred_class)\n",
    "\n",
    "        # А вот созданием датасета с предсказаниями \n",
    "        output = pd.DataFrame({\n",
    "            'image_name': names,\n",
    "            'predicted_class': model_result\n",
    "        })\n",
    "        # Сохраняем!\n",
    "        output.to_csv('/kaggle/working/submission11.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь пришло время подготовить данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/input/train-zip/train'\n",
    "\n",
    "data_idx = pd.read_csv('/kaggle/input/train-data/train.csv')\n",
    "img_to_class = {row['image_name']: row['class_id'] for _, row in data_idx.iterrows()}\n",
    "class_to_name = {row['class_id']: row['unified_class'] for _, row in data_idx.iterrows()}\n",
    "images_names = data_idx['image_name'].values\n",
    "imagest_target = [img_to_class[name] for name in images_names]\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_names.reshape(-1,1), imagest_target, test_size=0.2, stratify=imagest_target, random_state=42)\n",
    "\n",
    "train = VITdataset(path, X_train, y_train, trainable = True)\n",
    "val = VITdataset(path, X_test, y_test, trainable = False)\n",
    "\n",
    "train = DataLoader(train, batch_size=16, shuffle=True)\n",
    "val = DataLoader(val, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренируем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 7\n",
    "\n",
    "Vit = VIT\n",
    "Vit.train(train, val, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказываем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = '/kaggle/input/test-zip/test'\n",
    "Vit.predict(pred_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь можно посмотреть на датасет с предсказаниями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/working/submission11.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что помогло для решения задачи (и в принципе для погружения в ML/DL)\n",
    "\n",
    "- курсы Евгения Соколова (МО1 и МО2)\n",
    "\n",
    "- курс глубинного обучения от Ильдуса Садртдинова\n",
    "\n",
    "лекции выше читались для студентов 3 курса ПМИ ВШЭ\n",
    "\n",
    "- про VIT узнала, когда искала модели для улучшения решения на AIChallenge, \n",
    "но тогда остановилась на ResNet, поэтому в этот раз захотелось применить модель VIT и лучше в ней ней разобраться "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
